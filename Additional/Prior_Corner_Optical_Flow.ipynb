{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remarks\n",
    "\n",
    "* Data normalization\n",
    "    * Mobilenet expects data from -1 to 1\n",
    "        * Normalize Input Data or Include in Model\n",
    "        * TFLite Conversion must fit according to decision\n",
    "    * Ground Truth Data: for better inspection Data multiplied by 80. Undo the change in the Data Input Pipeline\n",
    "* Overview in Tutorials:\n",
    "    * tf.function\n",
    "* Idea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!for a in /sys/bus/pci/devices/*; do echo 0 | sudo tee -a $a/numa_node; done\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import yaml\n",
    "\n",
    "import DataProcessing.dataset as dataset\n",
    "import DataProcessing.model_files as model_files\n",
    "import Nets.backbones as backbones\n",
    "import Nets.features as features\n",
    "import Nets.losses as losses\n",
    "import Nets.metrics as metrics\n",
    "import Nets.visualize as visualize\n",
    "import Nets.tools as tools\n",
    "\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "\n",
    "#np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter Notebook\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--model', type=str, required=False, default=None)\n",
    "parser.add_argument('--data', type=str, required=False, default=None)\n",
    "\n",
    "parser.add_argument('--bs', type=int, required=False, default=None)\n",
    "parser.add_argument('--idx', type=int, required=False, default=None)\n",
    "parser.add_argument('--epoch', type=int, required=False, default=None)\n",
    "parser.add_argument('--noise', type=float, required=False, default=None)\n",
    "\n",
    "parser.add_argument('--train_model', action='store_true', default=False)\n",
    "parser.add_argument('--cache', action='store_true', default=False)\n",
    "parser.add_argument('--save', action='store_true', default=False)\n",
    "parser.add_argument('--sigmoid', action='store_true', default=False)\n",
    "parser.add_argument('--focal', action='store_true', default=False)\n",
    "\n",
    "parser.add_argument('--beta_upper', type=float, required=False, default=None)\n",
    "parser.add_argument('--gamma', type=float, required=False, default=None)\n",
    "parser.add_argument('--alpha', type=float, required=False, default=None)\n",
    "\n",
    "file_name = None\n",
    "try:\n",
    "    file_name = __file__\n",
    "except:\n",
    "    print(\"Jupyter Notebook\")\n",
    "       \n",
    "if file_name is None:\n",
    "    args = parser.parse_args(\"\")\n",
    "    args.train_model = True\n",
    "    args.cache = True\n",
    "    #args.save = True\n",
    "    args.save = True\n",
    "    args.sigmoid = False\n",
    "    args.focal = True\n",
    "else:    \n",
    "    args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generall Parameters\n",
    "TRAIN_MODEL = True #args.train_model\n",
    "SEED = None\n",
    "\n",
    "# LOSS\n",
    "weighted_multi_label_sigmoid_edge_loss = args.sigmoid\n",
    "# focal_loss = args.focal\n",
    "focal_loss = True\n",
    "\n",
    "beta_upper = 0.5 if args.beta_upper is None else args.beta_upper\n",
    "beta_lower = 1.0 - beta_upper\n",
    "gamma = 2.0 if args.gamma is None else args.gamma\n",
    "alpha = 2.0 if args.alpha is None else args.alpha\n",
    "class_weighted = True\n",
    "weighted_beta = True\n",
    "\n",
    "config_path = os.path.join(os.getcwd(), 'configs')\n",
    "model_config_path = os.path.join(config_path, 'model.yaml')\n",
    "\n",
    "with open(model_config_path, 'r') as file:\n",
    "    model_cfg = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset, Preprocess Images and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-02 12:59:40.754894: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-11-02 12:59:40.754943: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (david-ThinkPad-X1-Yoga-Gen-6): /proc/driver/nvidia/version does not exist\n",
      "2022-11-02 12:59:40.755924: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "TRAIN 1\n",
      "The TRAIN DS contains 202 images.\n",
      "0\n",
      "1\n",
      "TEST 2\n",
      "The TEST DS contains 2 images.\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(SEED)\n",
    "\n",
    "DP = dataset.DataProcessing(model_cfg[\"INPUT_SHAPE\"], model_cfg[\"OUTPUT_SHAPE\"], config_path)\n",
    "DP.path_definitions()\n",
    "\n",
    "MF = model_files.ModelFiles()\n",
    "MF.path_definitions(model_cfg[\"NAME\"], DP.ds_cfg[\"NAME\"], make_dirs=True)\n",
    "MF.clean_model_directories(model_cfg[\"CALLBACKS\"][\"DEL_OLD_CKPT\"], model_cfg[\"CALLBACKS\"][\"DEL_OLD_TB\"])\n",
    "\n",
    "if TRAIN_MODEL:\n",
    "    rng = tf.random.Generator.from_seed(123, alg='philox')\n",
    "\n",
    "    train_ds, img_count_train = DP.load_dataset(DP.key.train)\n",
    "    train_ds = DP.dataset_processing(train_ds, DP.key.train, shuffle=True, prefetch=True, img_count=img_count_train,\n",
    "                                     rng=rng)\n",
    "\n",
    "test_ds, img_count_test = DP.load_dataset(DP.key.test)\n",
    "test_ds = DP.dataset_processing(test_ds, DP.key.test, shuffle=False, prefetch=True, img_count=img_count_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_IMG (InputLayer)         [(None, 640, 360, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " input_PRIOR_ANN (InputLayer)   [(None, 160, 90, 1)  0           []                               \n",
      "                                ]                                                                 \n",
      "                                                                                                  \n",
      " tf.math.truediv (TFOpLambda)   (None, 640, 360, 3)  0           ['input_IMG[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 160, 90, 9)   81          ['input_PRIOR_ANN[0][0]']        \n",
      "                                                                                                  \n",
      " tf.math.subtract (TFOpLambda)  (None, 640, 360, 3)  0           ['tf.math.truediv[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 160, 90, 9)  36          ['conv2d[0][0]']                 \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " base_model (Functional)        [(None, 320, 180, 8  15664       ['tf.math.subtract[0][0]']       \n",
      "                                ),                                                                \n",
      "                                 (None, 160, 90, 8)                                               \n",
      "                                , (None, 80, 45, 16                                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " re_lu (ReLU)                   (None, 160, 90, 9)   0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " depthwise_conv2d (DepthwiseCon  (None, 80, 45, 9)   225         ['re_lu[0][0]']                  \n",
      " v2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 80, 45, 9)    1296        ['base_model[0][2]']             \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 80, 45, 9)   36          ['depthwise_conv2d[0][0]']       \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 80, 45, 9)   36          ['conv2d_1[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " x_out (ReLU)                   (None, 80, 45, 9)    0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " re_lu_1 (ReLU)                 (None, 80, 45, 9)    0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 80, 45, 18)   0           ['x_out[0][0]',                  \n",
      "                                                                  're_lu_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 80, 45, 9)    1458        ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 80, 45, 16)   2304        ['base_model[0][2]']             \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 80, 45, 9)   36          ['conv2d_2[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 80, 45, 16)  64          ['conv2d_4[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " re_lu_2 (ReLU)                 (None, 80, 45, 9)    0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " re_lu_4 (ReLU)                 (None, 80, 45, 16)   0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 80, 45, 9)    729         ['re_lu_2[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 80, 45, 9)    1296        ['re_lu_4[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 80, 45, 9)   36          ['conv2d_3[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 80, 45, 9)   36          ['conv2d_5[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " re_lu_3 (ReLU)                 (None, 80, 45, 9)    0           ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 160, 90, 9)   90          ['input_PRIOR_ANN[0][0]']        \n",
      "                                                                                                  \n",
      " out_mu (ReLU)                  (None, 80, 45, 9)    0           ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem (Slic  (None, 80, 45, 1)   0           ['re_lu_3[0][0]']                \n",
      " ingOpLambda)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_1 (Sl  (None, 80, 45, 1)   0           ['re_lu_3[0][0]']                \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_2 (Sl  (None, 80, 45, 1)   0           ['re_lu_3[0][0]']                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_3 (Sl  (None, 80, 45, 1)   0           ['re_lu_3[0][0]']                \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_4 (Sl  (None, 80, 45, 1)   0           ['re_lu_3[0][0]']                \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_5 (Sl  (None, 80, 45, 1)   0           ['re_lu_3[0][0]']                \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_6 (Sl  (None, 80, 45, 1)   0           ['re_lu_3[0][0]']                \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_7 (Sl  (None, 80, 45, 1)   0           ['re_lu_3[0][0]']                \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_8 (Sl  (None, 80, 45, 1)   0           ['re_lu_3[0][0]']                \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 160, 90, 9)  36          ['conv2d_12[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " tf.math.multiply (TFOpLambda)  (None, 80, 45, 9)    0           ['out_mu[0][0]',                 \n",
      "                                                                  'tf.__operators__.getitem[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_1 (TFOpLambda  (None, 80, 45, 9)   0           ['out_mu[0][0]',                 \n",
      " )                                                                'tf.__operators__.getitem_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " tf.math.multiply_2 (TFOpLambda  (None, 80, 45, 9)   0           ['out_mu[0][0]',                 \n",
      " )                                                                'tf.__operators__.getitem_2[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " tf.math.multiply_3 (TFOpLambda  (None, 80, 45, 9)   0           ['out_mu[0][0]',                 \n",
      " )                                                                'tf.__operators__.getitem_3[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " tf.math.multiply_4 (TFOpLambda  (None, 80, 45, 9)   0           ['out_mu[0][0]',                 \n",
      " )                                                                'tf.__operators__.getitem_4[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " tf.math.multiply_5 (TFOpLambda  (None, 80, 45, 9)   0           ['out_mu[0][0]',                 \n",
      " )                                                                'tf.__operators__.getitem_5[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " tf.math.multiply_6 (TFOpLambda  (None, 80, 45, 9)   0           ['out_mu[0][0]',                 \n",
      " )                                                                'tf.__operators__.getitem_6[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " tf.math.multiply_7 (TFOpLambda  (None, 80, 45, 9)   0           ['out_mu[0][0]',                 \n",
      " )                                                                'tf.__operators__.getitem_7[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " tf.math.multiply_8 (TFOpLambda  (None, 80, 45, 9)   0           ['out_mu[0][0]',                 \n",
      " )                                                                'tf.__operators__.getitem_8[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " re_lu_8 (ReLU)                 (None, 160, 90, 9)   0           ['batch_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 80, 45, 81)   0           ['tf.math.multiply[0][0]',       \n",
      "                                                                  'tf.math.multiply_1[0][0]',     \n",
      "                                                                  'tf.math.multiply_2[0][0]',     \n",
      "                                                                  'tf.math.multiply_3[0][0]',     \n",
      "                                                                  'tf.math.multiply_4[0][0]',     \n",
      "                                                                  'tf.math.multiply_5[0][0]',     \n",
      "                                                                  'tf.math.multiply_6[0][0]',     \n",
      "                                                                  'tf.math.multiply_7[0][0]',     \n",
      "                                                                  'tf.math.multiply_8[0][0]']     \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 160, 90, 9)   738         ['re_lu_8[0][0]']                \n",
      "                                                                                                  \n",
      " depthwise_conv2d_1 (DepthwiseC  (None, 80, 45, 81)  729         ['concatenate_1[0][0]']          \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 160, 90, 9)  36          ['conv2d_13[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 80, 45, 16)   11680       ['depthwise_conv2d_1[0][0]']     \n",
      "                                                                                                  \n",
      " re_lu_9 (ReLU)                 (None, 160, 90, 9)   0           ['batch_normalization_12[0][0]'] \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " batch_normalization_7 (BatchNo  (None, 80, 45, 16)  64          ['conv2d_6[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 160, 90, 9)   738         ['re_lu_9[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_5 (ReLU)                 (None, 80, 45, 16)   0           ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 160, 90, 9)  36          ['conv2d_14[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 80, 45, 9)    1305        ['re_lu_5[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_10 (ReLU)                (None, 160, 90, 9)   0           ['batch_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 80, 45, 9)   36          ['conv2d_11[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 80, 45, 9)    0           ['re_lu_10[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 80, 45, 18)   0           ['batch_normalization_10[0][0]', \n",
      "                                                                  'max_pooling2d[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 80, 45, 9)    1467        ['concatenate_3[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_14 (BatchN  (None, 80, 45, 9)   36          ['conv2d_15[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_11 (ReLU)                (None, 80, 45, 9)    0           ['batch_normalization_14[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 80, 45, 16)   1312        ['re_lu_11[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 80, 45, 16)  64          ['conv2d_16[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_12 (ReLU)                (None, 80, 45, 16)   0           ['batch_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)             (None, 80, 45, 16)   2320        ['re_lu_12[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 80, 45, 16)  64          ['conv2d_17[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " tf.image.resize (TFOpLambda)   (None, 160, 90, 16)  0           ['re_lu_5[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_13 (ReLU)                (None, 80, 45, 16)   0           ['batch_normalization_16[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 160, 90, 16)  2320        ['tf.image.resize[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 160, 90, 16)  1168        ['base_model[0][1]']             \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)             (None, 80, 45, 16)   2320        ['re_lu_13[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 160, 90, 16)  64         ['conv2d_7[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 160, 90, 16)  64         ['conv2d_9[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 80, 45, 16)  64          ['conv2d_18[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_6 (ReLU)                 (None, 160, 90, 16)  0           ['batch_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " re_lu_7 (ReLU)                 (None, 160, 90, 16)  0           ['batch_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " re_lu_14 (ReLU)                (None, 80, 45, 16)   0           ['batch_normalization_17[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 160, 90, 1)   145         ['re_lu_6[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 160, 90, 1)   145         ['re_lu_7[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate)    (None, 80, 45, 32)   0           ['re_lu_14[0][0]',               \n",
      "                                                                  're_lu_12[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 160, 90, 2)   0           ['conv2d_8[0][0]',               \n",
      "                                                                  'conv2d_10[0][0]']              \n",
      "                                                                                                  \n",
      " output_VERT_MAP (Conv2D)       (None, 80, 45, 2)    578         ['concatenate_4[0][0]']          \n",
      "                                                                                                  \n",
      " output_ANN (Conv2D)            (None, 160, 90, 1)   19          ['concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 50,971\n",
      "Trainable params: 48,997\n",
      "Non-trainable params: 1,974\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "if TRAIN_MODEL:\n",
    "    output_dims = model_cfg[\"OUTPUT_SHAPE\"]\n",
    "\n",
    "    # BACKBONE\n",
    "    backbone, output_names = backbones.get_backbone(name=model_cfg[\"BACKBONE\"][\"NAME\"],\n",
    "                                                    weights=model_cfg[\"BACKBONE\"][\"WEIGHTS\"],\n",
    "                                                    height=model_cfg[\"INPUT_SHAPE\"][0],\n",
    "                                                    width=model_cfg[\"INPUT_SHAPE\"][1],\n",
    "                                                    alpha=model_cfg[\"BACKBONE\"][\"ALPHA\"],\n",
    "                                                    output_layer=model_cfg[\"BACKBONE\"][\"OUTPUT_IDS\"],\n",
    "                                                    trainable_idx=model_cfg[\"BACKBONE\"][\"TRAIN_IDX\"])\n",
    "    \n",
    "    # prior edge map input\n",
    "    ann_input = tf.keras.Input(shape=(output_dims[0], output_dims[1],1), name='input_PRIOR_ANN')\n",
    "    x = tf.keras.layers.Conv2D(9, kernel_size=3, dilation_rate=1, padding=\"same\",\n",
    "                      strides=1, use_bias=False)(ann_input)\n",
    "    \n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "    \n",
    "    x = tf.keras.layers.DepthwiseConv2D(kernel_size=5, dilation_rate=1, padding=\"same\",\n",
    "                                        strides=2, use_bias=False)(x)\n",
    "    \n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x_out = tf.keras.layers.ReLU(name=\"x_out\")(x)\n",
    "    \n",
    "    # backbone output\n",
    "    b = tf.keras.layers.Conv2D(9, kernel_size=3, dilation_rate=1, padding=\"same\", \n",
    "                               strides=1, use_bias=False)(backbone.output[-1])\n",
    "    b = tf.keras.layers.BatchNormalization()(b)\n",
    "    b = tf.keras.layers.ReLU()(b)\n",
    "    \n",
    "    # Mix layers\n",
    "    #concat:\n",
    "    c = tf.keras.layers.Concatenate(axis=-1)([x_out,b])\n",
    "    c = tf.keras.layers.Conv2D(9, kernel_size=3, dilation_rate=1, padding=\"same\",\n",
    "                                    strides=1, use_bias=False)(c)\n",
    "    c = tf.keras.layers.BatchNormalization()(c)\n",
    "    c = tf.keras.layers.ReLU()(c)\n",
    "    c = tf.keras.layers.Conv2D(9, kernel_size=3, dilation_rate=1, padding=\"same\",\n",
    "                                strides=1, use_bias=False)(c)\n",
    "    c = tf.keras.layers.BatchNormalization()(c)\n",
    "    c = tf.keras.layers.ReLU()(c)\n",
    "    \n",
    "    # mult. layers:\n",
    "    mu = tf.keras.layers.Conv2D(16, kernel_size=3, dilation_rate=1, padding=\"same\", \n",
    "                           strides=1, use_bias=False)(backbone.output[-1])\n",
    "    mu = tf.keras.layers.BatchNormalization()(mu)\n",
    "    mu = tf.keras.layers.ReLU()(mu)\n",
    "    \n",
    "    mu = tf.keras.layers.Conv2D(9, kernel_size=3, dilation_rate=1, padding=\"same\", \n",
    "                       strides=1, use_bias=False)(mu)\n",
    "    mu = tf.keras.layers.BatchNormalization()(mu)\n",
    "    mu = tf.keras.layers.ReLU(name=\"out_mu\")(mu)\n",
    "    \n",
    "    out = []\n",
    "    for i in range(9):\n",
    "        out.append(mu*c[:,:,:,i:i+1])\n",
    "    x = tf.keras.layers.Concatenate(axis=-1)(out)\n",
    "    \n",
    "    # Post Processing\n",
    "    x = tf.keras.layers.DepthwiseConv2D(kernel_size=3, dilation_rate=1, padding=\"same\",\n",
    "                                        strides=1, use_bias=False)(x)\n",
    "    \n",
    "    x = tf.keras.layers.Conv2D(16, kernel_size=3, dilation_rate=1, padding=\"same\",\n",
    "                               strides=1, use_bias=True)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "\n",
    "    x_resize = tf.image.resize(x, (output_dims[0], output_dims[1]))\n",
    "    x_edge = tf.keras.layers.Conv2D(16, kernel_size=3, dilation_rate=1, padding=\"same\",\n",
    "                  strides=1, use_bias=True)(x_resize)\n",
    "    x_edge = tf.keras.layers.BatchNormalization()(x_edge)\n",
    "\n",
    "    x_edge = tf.keras.layers.ReLU()(x_edge)\n",
    "\n",
    "    x_edge = tf.keras.layers.Conv2D(1, kernel_size=3, dilation_rate=1, padding=\"same\",\n",
    "                                    strides=1, use_bias=True)(x_edge)\n",
    "    edge_map = tf.keras.layers.Conv2D(16, kernel_size=3, dilation_rate=1, padding=\"same\",\n",
    "                                strides=1, use_bias=True)(backbone.output[1])\n",
    "\n",
    "    edge_map = tf.keras.layers.BatchNormalization()(edge_map)\n",
    "\n",
    "    edge_map = tf.keras.layers.ReLU()(edge_map)\n",
    "\n",
    "    edge_map = tf.keras.layers.Conv2D(1, kernel_size=3, dilation_rate=1, padding=\"same\",\n",
    "                                strides=1, use_bias=True)(edge_map)\n",
    "\n",
    "    x_edge = tf.keras.layers.Concatenate(axis=-1)([x_edge, edge_map])\n",
    "\n",
    "    output_edge = tf.keras.layers.Conv2D(1, kernel_size=3, dilation_rate=1, padding=\"same\",\n",
    "                                        strides=1, use_bias=True, name=\"output_ANN\")(x_edge)\n",
    "    \n",
    "    comp = tf.keras.layers.Conv2D(9, kernel_size=3, dilation_rate=1, padding=\"same\", \n",
    "                                 strides=1, use_bias=True)(x)\n",
    "    comp = tf.keras.layers.BatchNormalization()(comp)\n",
    "    \n",
    "    edge_comp = tf.keras.layers.Conv2D(9, kernel_size=3, dilation_rate=1, padding=\"same\", \n",
    "                                 strides=1, use_bias=True)(ann_input)\n",
    "    \n",
    "    edge_comp = tf.keras.layers.BatchNormalization()(edge_comp)\n",
    "    edge_comp = tf.keras.layers.ReLU()(edge_comp)\n",
    "    \n",
    "    edge_comp = tf.keras.layers.Conv2D(9, kernel_size=3, dilation_rate=1, padding=\"same\", \n",
    "                                       strides=1, use_bias=True)(edge_comp)\n",
    "    edge_comp = tf.keras.layers.BatchNormalization()(edge_comp)\n",
    "    edge_comp = tf.keras.layers.ReLU()(edge_comp)\n",
    "    \n",
    "    edge_comp = tf.keras.layers.Conv2D(9, kernel_size=3, dilation_rate=1, padding=\"same\", \n",
    "                                   strides=1, use_bias=True)(edge_comp)\n",
    "    edge_comp = tf.keras.layers.BatchNormalization()(edge_comp)\n",
    "    edge_comp = tf.keras.layers.ReLU()(edge_comp)\n",
    "    \n",
    "    edge_comp = tf.keras.layers.MaxPool2D(pool_size=(3, 3),strides=2,padding='same')(edge_comp)\n",
    "    \n",
    "    comp = tf.keras.layers.Concatenate(axis=-1)([comp, edge_comp])\n",
    "    comp = tf.keras.layers.Conv2D(9, kernel_size=3, dilation_rate=1, padding=\"same\", \n",
    "                               strides=1, use_bias=True)(comp)\n",
    "    comp = tf.keras.layers.BatchNormalization()(comp)\n",
    "    comp = tf.keras.layers.ReLU()(comp)\n",
    "    \n",
    "    comp = tf.keras.layers.Conv2D(16, kernel_size=3, dilation_rate=1, padding=\"same\", \n",
    "                           strides=1, use_bias=True)(comp)\n",
    "    comp = tf.keras.layers.BatchNormalization()(comp)\n",
    "    comp_sep = tf.keras.layers.ReLU()(comp)\n",
    "    \n",
    "    comp = tf.keras.layers.Conv2D(16, kernel_size=3, dilation_rate=1, padding=\"same\", \n",
    "                       strides=1, use_bias=True)(comp_sep)\n",
    "    comp = tf.keras.layers.BatchNormalization()(comp)\n",
    "    comp = tf.keras.layers.ReLU()(comp)\n",
    "    \n",
    "    comp = tf.keras.layers.Conv2D(16, kernel_size=3, dilation_rate=1, padding=\"same\", \n",
    "                   strides=1, use_bias=True)(comp)\n",
    "    comp = tf.keras.layers.BatchNormalization()(comp)\n",
    "    comp = tf.keras.layers.ReLU()(comp)\n",
    "    \n",
    "    comp = tf.keras.layers.Concatenate(axis=-1)([comp, comp_sep])\n",
    "    \n",
    "    comp = tf.keras.layers.Conv2D(2, kernel_size=3, dilation_rate=1, padding=\"same\", name=\"output_VERT_MAP\")(comp)\n",
    "\n",
    "\n",
    "    model = tf.keras.Model(inputs=(backbone.input, ann_input), outputs=[comp, output_edge])\n",
    "\n",
    "    model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/120\n",
      "201/201 [==============================] - 26s 117ms/step - loss: 46.4495 - output_VERT_MAP_loss: 46.3197 - output_ANN_loss: 129.7484 - val_loss: 143.2616 - val_output_VERT_MAP_loss: 143.0425 - val_output_ANN_loss: 219.0947\n",
      "Epoch 2/120\n",
      "201/201 [==============================] - 27s 132ms/step - loss: 43.6205 - output_VERT_MAP_loss: 43.4906 - output_ANN_loss: 129.8857 - val_loss: 147.2022 - val_output_VERT_MAP_loss: 146.9726 - val_output_ANN_loss: 229.6655\n",
      "Epoch 3/120\n",
      "201/201 [==============================] - 27s 132ms/step - loss: 40.8416 - output_VERT_MAP_loss: 40.7152 - output_ANN_loss: 126.4077 - val_loss: 136.0210 - val_output_VERT_MAP_loss: 135.8336 - val_output_ANN_loss: 187.4046\n",
      "Epoch 4/120\n",
      "201/201 [==============================] - 27s 133ms/step - loss: 39.4138 - output_VERT_MAP_loss: 39.2863 - output_ANN_loss: 127.5168 - val_loss: 134.1736 - val_output_VERT_MAP_loss: 134.0113 - val_output_ANN_loss: 162.3262\n",
      "Epoch 5/120\n",
      "201/201 [==============================] - 46s 226ms/step - loss: 38.3135 - output_VERT_MAP_loss: 38.1862 - output_ANN_loss: 127.3408 - val_loss: 141.8502 - val_output_VERT_MAP_loss: 141.6810 - val_output_ANN_loss: 169.1850\n",
      "Epoch 6/120\n",
      "201/201 [==============================] - 30s 147ms/step - loss: 36.4847 - output_VERT_MAP_loss: 36.3572 - output_ANN_loss: 127.4644 - val_loss: 132.5825 - val_output_VERT_MAP_loss: 132.4376 - val_output_ANN_loss: 144.8498\n",
      "Epoch 7/120\n",
      "201/201 [==============================] - 27s 133ms/step - loss: 35.0466 - output_VERT_MAP_loss: 34.9183 - output_ANN_loss: 128.3479 - val_loss: 124.3255 - val_output_VERT_MAP_loss: 124.1857 - val_output_ANN_loss: 139.7621\n",
      "Epoch 8/120\n",
      "201/201 [==============================] - 28s 136ms/step - loss: 34.2774 - output_VERT_MAP_loss: 34.1501 - output_ANN_loss: 127.2812 - val_loss: 117.4350 - val_output_VERT_MAP_loss: 117.3064 - val_output_ANN_loss: 128.5919\n",
      "Epoch 9/120\n",
      "201/201 [==============================] - 28s 139ms/step - loss: 32.6599 - output_VERT_MAP_loss: 32.5310 - output_ANN_loss: 128.8701 - val_loss: 117.9631 - val_output_VERT_MAP_loss: 117.8178 - val_output_ANN_loss: 145.2627\n",
      "Epoch 10/120\n",
      "201/201 [==============================] - 37s 181ms/step - loss: 32.2732 - output_VERT_MAP_loss: 32.1449 - output_ANN_loss: 128.2972 - val_loss: 115.5088 - val_output_VERT_MAP_loss: 115.3629 - val_output_ANN_loss: 145.9108\n",
      "Epoch 11/120\n",
      "201/201 [==============================] - 26s 131ms/step - loss: 30.9540 - output_VERT_MAP_loss: 30.8260 - output_ANN_loss: 127.9693 - val_loss: 106.3244 - val_output_VERT_MAP_loss: 106.1930 - val_output_ANN_loss: 131.3400\n",
      "Epoch 12/120\n",
      "201/201 [==============================] - 28s 137ms/step - loss: 30.2047 - output_VERT_MAP_loss: 30.0737 - output_ANN_loss: 131.0238 - val_loss: 107.1366 - val_output_VERT_MAP_loss: 106.9954 - val_output_ANN_loss: 141.2691\n",
      "Epoch 13/120\n",
      "201/201 [==============================] - 27s 134ms/step - loss: 29.5584 - output_VERT_MAP_loss: 29.4301 - output_ANN_loss: 128.3186 - val_loss: 105.6079 - val_output_VERT_MAP_loss: 105.4576 - val_output_ANN_loss: 150.2914\n",
      "Epoch 14/120\n",
      "201/201 [==============================] - 38s 190ms/step - loss: 29.1640 - output_VERT_MAP_loss: 29.0344 - output_ANN_loss: 129.6868 - val_loss: 105.0997 - val_output_VERT_MAP_loss: 104.9529 - val_output_ANN_loss: 146.7966\n",
      "Epoch 15/120\n",
      "201/201 [==============================] - 31s 152ms/step - loss: 28.2526 - output_VERT_MAP_loss: 28.1220 - output_ANN_loss: 130.5647 - val_loss: 116.2238 - val_output_VERT_MAP_loss: 116.0660 - val_output_ANN_loss: 157.7815\n",
      "Epoch 16/120\n",
      "201/201 [==============================] - 28s 141ms/step - loss: 27.4830 - output_VERT_MAP_loss: 27.3529 - output_ANN_loss: 130.0970 - val_loss: 105.8910 - val_output_VERT_MAP_loss: 105.7394 - val_output_ANN_loss: 151.5703\n",
      "Epoch 17/120\n",
      "201/201 [==============================] - 28s 137ms/step - loss: 26.2360 - output_VERT_MAP_loss: 26.1046 - output_ANN_loss: 131.3806 - val_loss: 97.1423 - val_output_VERT_MAP_loss: 96.9777 - val_output_ANN_loss: 164.5606\n",
      "Epoch 18/120\n",
      "201/201 [==============================] - 38s 190ms/step - loss: 25.1906 - output_VERT_MAP_loss: 25.0603 - output_ANN_loss: 130.3221 - val_loss: 94.0505 - val_output_VERT_MAP_loss: 93.9021 - val_output_ANN_loss: 148.3851\n",
      "Epoch 19/120\n",
      "201/201 [==============================] - 28s 137ms/step - loss: 24.3722 - output_VERT_MAP_loss: 24.2394 - output_ANN_loss: 132.7685 - val_loss: 92.6455 - val_output_VERT_MAP_loss: 92.4912 - val_output_ANN_loss: 154.3043\n",
      "Epoch 20/120\n",
      "201/201 [==============================] - 28s 137ms/step - loss: 24.6336 - output_VERT_MAP_loss: 24.5005 - output_ANN_loss: 133.0325 - val_loss: 93.1073 - val_output_VERT_MAP_loss: 92.9487 - val_output_ANN_loss: 158.5758\n",
      "Epoch 21/120\n",
      "201/201 [==============================] - 28s 138ms/step - loss: 23.9693 - output_VERT_MAP_loss: 23.8374 - output_ANN_loss: 131.8917 - val_loss: 93.9421 - val_output_VERT_MAP_loss: 93.7763 - val_output_ANN_loss: 165.7957\n",
      "Epoch 22/120\n",
      "201/201 [==============================] - 39s 192ms/step - loss: 23.6243 - output_VERT_MAP_loss: 23.4930 - output_ANN_loss: 131.3168 - val_loss: 92.5234 - val_output_VERT_MAP_loss: 92.3685 - val_output_ANN_loss: 154.9201\n",
      "Epoch 23/120\n",
      "201/201 [==============================] - 28s 138ms/step - loss: 22.8440 - output_VERT_MAP_loss: 22.7123 - output_ANN_loss: 131.7116 - val_loss: 95.4116 - val_output_VERT_MAP_loss: 95.2525 - val_output_ANN_loss: 159.1083\n",
      "Epoch 24/120\n",
      "201/201 [==============================] - 28s 138ms/step - loss: 21.8633 - output_VERT_MAP_loss: 21.7318 - output_ANN_loss: 131.4594 - val_loss: 92.6878 - val_output_VERT_MAP_loss: 92.5398 - val_output_ANN_loss: 148.0088\n",
      "Epoch 25/120\n",
      "201/201 [==============================] - 27s 135ms/step - loss: 22.2111 - output_VERT_MAP_loss: 22.0775 - output_ANN_loss: 133.5403 - val_loss: 91.8027 - val_output_VERT_MAP_loss: 91.6568 - val_output_ANN_loss: 145.9306\n",
      "Epoch 26/120\n",
      "201/201 [==============================] - 37s 183ms/step - loss: 21.7048 - output_VERT_MAP_loss: 21.5703 - output_ANN_loss: 134.4735 - val_loss: 90.1703 - val_output_VERT_MAP_loss: 90.0128 - val_output_ANN_loss: 157.4714\n",
      "Epoch 27/120\n",
      "201/201 [==============================] - 29s 144ms/step - loss: 21.1249 - output_VERT_MAP_loss: 20.9920 - output_ANN_loss: 132.8343 - val_loss: 86.0323 - val_output_VERT_MAP_loss: 85.8919 - val_output_ANN_loss: 140.4507\n",
      "Epoch 28/120\n",
      "201/201 [==============================] - 27s 135ms/step - loss: 20.7451 - output_VERT_MAP_loss: 20.6114 - output_ANN_loss: 133.7217 - val_loss: 88.8636 - val_output_VERT_MAP_loss: 88.7058 - val_output_ANN_loss: 157.7969\n",
      "Epoch 29/120\n",
      "201/201 [==============================] - 28s 137ms/step - loss: 21.2345 - output_VERT_MAP_loss: 21.1005 - output_ANN_loss: 134.0014 - val_loss: 79.5672 - val_output_VERT_MAP_loss: 79.4191 - val_output_ANN_loss: 148.0706\n",
      "Epoch 30/120\n",
      "201/201 [==============================] - 35s 174ms/step - loss: 20.4771 - output_VERT_MAP_loss: 20.3422 - output_ANN_loss: 134.9515 - val_loss: 92.5047 - val_output_VERT_MAP_loss: 92.3525 - val_output_ANN_loss: 152.1422\n",
      "Epoch 31/120\n",
      "201/201 [==============================] - 29s 144ms/step - loss: 19.3137 - output_VERT_MAP_loss: 19.1805 - output_ANN_loss: 133.1188 - val_loss: 93.8504 - val_output_VERT_MAP_loss: 93.6916 - val_output_ANN_loss: 158.8183\n",
      "Epoch 32/120\n",
      "201/201 [==============================] - 27s 136ms/step - loss: 19.2859 - output_VERT_MAP_loss: 19.1494 - output_ANN_loss: 136.5201 - val_loss: 85.2438 - val_output_VERT_MAP_loss: 85.0801 - val_output_ANN_loss: 163.6859\n",
      "Epoch 33/120\n",
      "201/201 [==============================] - 28s 137ms/step - loss: 18.8899 - output_VERT_MAP_loss: 18.7566 - output_ANN_loss: 133.2648 - val_loss: 93.5786 - val_output_VERT_MAP_loss: 93.4194 - val_output_ANN_loss: 159.2516\n",
      "Epoch 34/120\n",
      "201/201 [==============================] - 37s 185ms/step - loss: 18.1003 - output_VERT_MAP_loss: 17.9661 - output_ANN_loss: 134.2467 - val_loss: 91.3597 - val_output_VERT_MAP_loss: 91.2142 - val_output_ANN_loss: 145.4541\n",
      "Epoch 35/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201/201 [==============================] - 27s 135ms/step - loss: 18.9680 - output_VERT_MAP_loss: 18.8348 - output_ANN_loss: 133.1559 - val_loss: 89.3827 - val_output_VERT_MAP_loss: 89.2326 - val_output_ANN_loss: 150.0405\n",
      "Epoch 36/120\n",
      "201/201 [==============================] - 27s 136ms/step - loss: 18.4433 - output_VERT_MAP_loss: 18.3058 - output_ANN_loss: 137.4783 - val_loss: 89.3369 - val_output_VERT_MAP_loss: 89.1832 - val_output_ANN_loss: 153.6933\n",
      "Epoch 37/120\n",
      "201/201 [==============================] - 29s 141ms/step - loss: 17.4767 - output_VERT_MAP_loss: 17.3417 - output_ANN_loss: 134.9799 - val_loss: 87.0450 - val_output_VERT_MAP_loss: 86.8904 - val_output_ANN_loss: 154.5412\n",
      "Epoch 38/120\n",
      "201/201 [==============================] - 35s 175ms/step - loss: 17.5038 - output_VERT_MAP_loss: 17.3703 - output_ANN_loss: 133.4962 - val_loss: 79.8005 - val_output_VERT_MAP_loss: 79.6445 - val_output_ANN_loss: 156.0231\n",
      "Epoch 39/120\n",
      "201/201 [==============================] - 27s 133ms/step - loss: 17.7172 - output_VERT_MAP_loss: 17.5835 - output_ANN_loss: 133.6964 - val_loss: 88.2364 - val_output_VERT_MAP_loss: 88.0781 - val_output_ANN_loss: 158.3496\n",
      "Epoch 40/120\n",
      "201/201 [==============================] - 27s 134ms/step - loss: 17.1235 - output_VERT_MAP_loss: 16.9873 - output_ANN_loss: 136.2166 - val_loss: 89.4214 - val_output_VERT_MAP_loss: 89.2515 - val_output_ANN_loss: 169.9021\n",
      "Epoch 41/120\n",
      "201/201 [==============================] - 30s 147ms/step - loss: 17.1315 - output_VERT_MAP_loss: 16.9940 - output_ANN_loss: 137.5617 - val_loss: 81.9767 - val_output_VERT_MAP_loss: 81.8121 - val_output_ANN_loss: 164.5735\n",
      "Epoch 42/120\n",
      "201/201 [==============================] - 36s 178ms/step - loss: 16.6325 - output_VERT_MAP_loss: 16.4986 - output_ANN_loss: 133.8643 - val_loss: 87.4965 - val_output_VERT_MAP_loss: 87.3124 - val_output_ANN_loss: 184.1418\n",
      "Epoch 43/120\n",
      "201/201 [==============================] - 27s 134ms/step - loss: 16.3989 - output_VERT_MAP_loss: 16.2633 - output_ANN_loss: 135.6117 - val_loss: 81.9302 - val_output_VERT_MAP_loss: 81.7620 - val_output_ANN_loss: 168.2112\n",
      "Epoch 44/120\n",
      "201/201 [==============================] - 27s 136ms/step - loss: 15.9372 - output_VERT_MAP_loss: 15.7994 - output_ANN_loss: 137.8699 - val_loss: 88.2488 - val_output_VERT_MAP_loss: 88.0771 - val_output_ANN_loss: 171.7310\n",
      "Epoch 45/120\n",
      "201/201 [==============================] - 28s 137ms/step - loss: 16.0251 - output_VERT_MAP_loss: 15.8874 - output_ANN_loss: 137.6615 - val_loss: 85.3899 - val_output_VERT_MAP_loss: 85.2234 - val_output_ANN_loss: 166.4722\n",
      "Epoch 46/120\n",
      "201/201 [==============================] - 39s 195ms/step - loss: 15.3975 - output_VERT_MAP_loss: 15.2614 - output_ANN_loss: 136.0084 - val_loss: 83.0943 - val_output_VERT_MAP_loss: 82.9402 - val_output_ANN_loss: 154.1398\n",
      "Epoch 47/120\n",
      "201/201 [==============================] - 28s 136ms/step - loss: 15.7834 - output_VERT_MAP_loss: 15.6487 - output_ANN_loss: 134.6438 - val_loss: 87.1361 - val_output_VERT_MAP_loss: 86.9808 - val_output_ANN_loss: 155.2873\n",
      "Epoch 48/120\n",
      "201/201 [==============================] - 28s 137ms/step - loss: 15.0141 - output_VERT_MAP_loss: 14.8756 - output_ANN_loss: 138.5733 - val_loss: 85.4940 - val_output_VERT_MAP_loss: 85.3280 - val_output_ANN_loss: 165.9889\n",
      "Epoch 49/120\n",
      "201/201 [==============================] - 39s 192ms/step - loss: 14.9371 - output_VERT_MAP_loss: 14.8020 - output_ANN_loss: 135.0814 - val_loss: 94.0849 - val_output_VERT_MAP_loss: 93.9142 - val_output_ANN_loss: 170.7461\n",
      "Epoch 50/120\n",
      "201/201 [==============================] - 27s 134ms/step - loss: 14.6399 - output_VERT_MAP_loss: 14.5018 - output_ANN_loss: 138.0270 - val_loss: 85.5993 - val_output_VERT_MAP_loss: 85.4298 - val_output_ANN_loss: 169.4718\n",
      "Epoch 51/120\n",
      "201/201 [==============================] - 27s 133ms/step - loss: 14.6002 - output_VERT_MAP_loss: 14.4635 - output_ANN_loss: 136.7110 - val_loss: 82.7570 - val_output_VERT_MAP_loss: 82.6028 - val_output_ANN_loss: 154.2314\n",
      "Epoch 52/120\n",
      "201/201 [==============================] - 27s 134ms/step - loss: 14.3239 - output_VERT_MAP_loss: 14.1885 - output_ANN_loss: 135.4683 - val_loss: 84.1105 - val_output_VERT_MAP_loss: 83.9648 - val_output_ANN_loss: 145.6756\n",
      "Epoch 53/120\n",
      "201/201 [==============================] - 37s 184ms/step - loss: 13.5457 - output_VERT_MAP_loss: 13.4089 - output_ANN_loss: 136.8836 - val_loss: 87.9865 - val_output_VERT_MAP_loss: 87.8304 - val_output_ANN_loss: 156.1875\n",
      "Epoch 54/120\n",
      "201/201 [==============================] - 27s 134ms/step - loss: 13.5796 - output_VERT_MAP_loss: 13.4420 - output_ANN_loss: 137.5703 - val_loss: 91.1269 - val_output_VERT_MAP_loss: 90.9632 - val_output_ANN_loss: 163.6570\n",
      "Epoch 55/120\n",
      "201/201 [==============================] - 28s 140ms/step - loss: 13.6920 - output_VERT_MAP_loss: 13.5550 - output_ANN_loss: 136.9836 - val_loss: 86.1266 - val_output_VERT_MAP_loss: 85.9608 - val_output_ANN_loss: 165.8559\n",
      "Epoch 56/120\n",
      "124/201 [=================>............] - ETA: 10s - loss: 15.1661 - output_VERT_MAP_loss: 15.0342 - output_ANN_loss: 131.8542"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39mlr_schedule),\n\u001b[1;32m     20\u001b[0m               loss\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_ANN\u001b[39m\u001b[38;5;124m'\u001b[39m: losses\u001b[38;5;241m.\u001b[39mfocal_loss_binary,\n\u001b[1;32m     21\u001b[0m                     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_VERT_MAP\u001b[39m\u001b[38;5;124m'\u001b[39m: losses\u001b[38;5;241m.\u001b[39moptical_flow_loss},\n\u001b[1;32m     22\u001b[0m               loss_weights\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_VERT_MAP\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_ANN\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.001\u001b[39m})\n\u001b[1;32m     23\u001b[0m               \u001b[38;5;66;03m#metrics={'output_CURRENT_VERT': [metrics.BinaryAccuracyEdges(threshold_prediction=0),\u001b[39;00m\n\u001b[1;32m     24\u001b[0m               \u001b[38;5;66;03m#         metrics.F1Edges(threshold_prediction=0, threshold_edge_width=0)]})\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_cfg\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEPOCHS\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/SemesterProject/tf/lib/python3.8/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/SemesterProject/tf/lib/python3.8/site-packages/keras/engine/training.py:1384\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1377\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1378\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1379\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   1380\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[1;32m   1381\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1382\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m   1383\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1384\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1385\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1386\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/SemesterProject/tf/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/SemesterProject/tf/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/SemesterProject/tf/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/SemesterProject/tf/lib/python3.8/site-packages/tensorflow/python/eager/function.py:2956\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2953\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2954\u001b[0m   (graph_function,\n\u001b[1;32m   2955\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2956\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/SemesterProject/tf/lib/python3.8/site-packages/tensorflow/python/eager/function.py:1853\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1849\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1850\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1851\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1852\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1853\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1854\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1855\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m     args,\n\u001b[1;32m   1857\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1858\u001b[0m     executing_eagerly)\n\u001b[1;32m   1859\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/SemesterProject/tf/lib/python3.8/site-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/SemesterProject/tf/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if TRAIN_MODEL:\n",
    "    # learning rate schedule\n",
    "    base_learning_rate = 0.0015*0.1\n",
    "    end_learning_rate = 0.0005*0.1\n",
    "    decay_step = np.ceil(img_count_train / DP.ds_cfg[DP.key.train][\"BATCH_SIZE\"]) * model_cfg[\"EPOCHS\"]\n",
    "    lr_schedule = tf.keras.optimizers.schedules.PolynomialDecay(base_learning_rate, decay_steps=decay_step,\n",
    "                                                                end_learning_rate=end_learning_rate, power=0.9)\n",
    "    \n",
    "    frequency = int(\n",
    "        np.ceil(img_count_train / DP.ds_cfg[DP.key.train][\"BATCH_SIZE\"]) * model_cfg[\"CALLBACKS\"][\"CKPT_FREQ\"]) + 1\n",
    "\n",
    "    logdir = os.path.join(MF.paths['TBLOGS'], datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "    callbacks = [tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=MF.paths[\"CKPT\"] + \"/ckpt-loss={val_loss:.2f}-epoch={epoch:.2f}\",\n",
    "        save_weights_only=False, save_best_only=False, monitor=\"val_f1\", verbose=1, save_freq='epoch', period=frequency),\n",
    "        tf.keras.callbacks.TensorBoard(log_dir=logdir, histogram_freq=1)]\n",
    "\n",
    "    # compile model\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "                  loss={'output_ANN': losses.focal_loss_binary,\n",
    "                        'output_VERT_MAP': losses.optical_flow_loss},\n",
    "                  loss_weights={\"output_VERT_MAP\": 1.0, \"output_ANN\": 0.001})\n",
    "                  #metrics={'output_CURRENT_VERT': [metrics.BinaryAccuracyEdges(threshold_prediction=0),\n",
    "                  #         metrics.F1Edges(threshold_prediction=0, threshold_edge_width=0)]})\n",
    "\n",
    "    history = model.fit(train_ds, epochs=model_cfg[\"EPOCHS\"], validation_data=test_ds, callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckpt = os.listdir(MF.paths['CKPT'])\n",
    "\n",
    "f1_max = 0\n",
    "for ckpt_name in model_ckpt:\n",
    "    if float(ckpt_name[-4:]) > f1_max:\n",
    "        f1_max = float(ckpt_name[-4:])\n",
    "        model_path = MF.paths['CKPT'] + \"/\" + ckpt_name\n",
    "\n",
    "        print(model_path)\n",
    "\n",
    "custom_objects = {\"BinaryAccuracyEdges\": metrics.BinaryAccuracyEdges, \"F1Edges\": metrics.F1Edges}\n",
    "\n",
    "model = tf.keras.models.load_model(model_path, custom_objects=custom_objects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_MODEL:\n",
    "    plot_losses = [\"loss\", \"loss\"]\n",
    "    plot_metrics = [\"accuracy_edges\", \"f1\", \"recall\", \"precision\"]\n",
    "\n",
    "    path = os.path.join(MF.paths[\"FIGURES\"], \"training.svg\")\n",
    "\n",
    "    visualize.plot_training_results(res=history.history, losses=plot_losses, metrics=plot_metrics,\n",
    "                                    save=model_cfg[\"SAVE\"], path=path)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Maximum F1 Score:\n",
    "if not TRAIN_MODEL:\n",
    "    step_width = 0.05\n",
    "    threshold_range = [0.05,0.95]\n",
    "    threshold_array = np.arange(threshold_range[0],threshold_range[1]+step_width,step_width)\n",
    "    threshold_array = np.array([0.025, 0.1, 0.2,0.3,0.4,0.45,0.5,0.55,0.6,0.7,0.8, 0.9, 0.975])\n",
    "\n",
    "    path_metrics_evaluation_plot = os.path.join(paths[\"FIGURES\"],\"threshold_metrics_evaluation_test_ds.svg\")\n",
    "    threshold_f1_max = visualize.plot_threshold_metrics_evaluation_class(model=model, \n",
    "                                                                         ds=test_ds,\n",
    "                                                                         num_classes=NUM_CLASSES,\n",
    "                                                                         threshold_array=threshold_array, \n",
    "                                                                         threshold_edge_width=0, save=SAVE, \n",
    "                                                                         path=path_metrics_evaluation_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Maximum F1 Score:\n",
    "# TODO: FAILS right now: fix\n",
    "# if not TRAIN_MODEL:\n",
    "#     step_width = 0.05\n",
    "#     threshold_range = [0.05, 0.95]\n",
    "#     threshold_array = np.arange(threshold_range[0], threshold_range[1] + step_width, step_width)\n",
    "#     threshold_array = np.array([0.025, 0.1, 0.2, 0.3, 0.4, 0.45, 0.5, 0.55, 0.6, 0.7, 0.8, 0.9, 0.975])\n",
    "#\n",
    "#     path_metrics_evaluation_plot = os.path.join(MF.paths[\"FIGURES\"], \"threshold_metrics_evaluation_test_ds.svg\")\n",
    "#     threshold_f1_max = visualize.plot_threshold_metrics_evaluation_class(model=model, ds=test_ds,\n",
    "#                                                                          num_classes=DP.num_classes,\n",
    "#                                                                          threshold_array=threshold_array,\n",
    "#                                                                          threshold_edge_width=0, save=model_cfg[\"SAVE\"],\n",
    "#                                                                          path=path_metrics_evaluation_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "if FINE_TUNING and TRAIN_MODEL:\n",
    "\n",
    "    # Fine-tune from this layer onwards\n",
    "    fine_tune_output = output_names[1-1]\n",
    "\n",
    "    model.trainable = True\n",
    "\n",
    "    # Freeze all the layers before the `fine_tune_at` layer: \n",
    "    for submodel in model.layers:\n",
    "        if submodel.name == \"base_model\":\n",
    "            for layer in submodel.layers:\n",
    "                layer.trainable = False\n",
    "                if layer.name == fine_tune_output:\n",
    "                    break\n",
    "    \n",
    "    \n",
    "    total_epochs =  EPOCHS + FINE_TUNE_EPOCHS\n",
    "\n",
    "    base_learning_rate = 0.00001\n",
    "    end_learning_rate =  0.00001\n",
    "    decay_step = np.floor(img_count_train / BATCH_SIZE)*FINE_TUNE_EPOCHS\n",
    "    lr_schedule = tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "        base_learning_rate,decay_steps = decay_step,end_learning_rate = end_learning_rate, power = 0.9)\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "                  loss=loss,\n",
    "                  metrics={'output': [metrics.BinaryAccuracyEdges(threshold_prediction=0),\n",
    "                                      metrics.F1Edges(threshold_prediction=0, threshold_edge_width=0)]})\n",
    "    \n",
    "\n",
    "    history_fine = model.fit(train_ds, epochs=total_epochs, \n",
    "                               initial_epoch=history.epoch[-1]+1,validation_data=train_ds.take(1), \n",
    "                               callbacks=callbacks)\n",
    "    \n",
    "    plot_losses = [\"loss\", \"output_loss\"]\n",
    "    plot_metrics = [\"output_accuracy_edges\", \"f1\", \"recall\", \"precision\"]\n",
    "    \n",
    "    path = os.path.join(paths[\"FIGURES\"],\"fine_tuning_training.svg\")\n",
    "    \n",
    "    visualize.plot_training_results(res=history.history, res_fine = history_fine.history, \n",
    "                                losses=plot_losses, metrics=plot_metrics, save=SAVE, path=path, epochs=EPOCHS)\n",
    "    \n",
    "    path_metrics_evaluation_plot = os.path.join(paths[\"FIGURES\"],\"threshold_metrics_evaluation__fine_tune_test_ds.svg\")\n",
    "    visualize.plot_threshold_metrics_evaluation(model=model, ds=test_ds, threshold_array=threshold_array, \n",
    "                                        threshold_edge_width=0, save=SAVE, path=path_metrics_evaluation_plot, \n",
    "                                        accuracy_y_lim_min = 0.9)\n",
    "        \n",
    "    for img, label in test_ds.take(1):\n",
    "        img, label = img, label\n",
    "\n",
    "    predictions = model.predict(img)    \n",
    "    predictions = tools.predict_class_postprocessing(predictions[0], threshold = 0.5)\n",
    "\n",
    "    path = os.path.join(paths[\"FIGURES\"],\"fine_tuning_images_0,5\")\n",
    "    visualize.plot_images(images=img, labels=label, predictions=predictions, save=SAVE, path=path, batch_size=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on Test DS of Real Images"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "DATA_REAL = 'RealRed'\n",
    "TRAIN_REAL = 'Train'\n",
    "TEST_REAL = 'Test'\n",
    "TEST_HARD_REAL = 'Test Hard'\n",
    "IMG_ONLY_REAL = 'Img Only'\n",
    "BS_REAL = 8\n",
    "\n",
    "paths_real, files_real = data_processing.path_definitions(MODEL, DATA_REAL, TRAIN_REAL, TEST_REAL, TEST_HARD_REAL, IMG_ONLY_REAL, make_dirs=False)\n",
    "\n",
    "test_real_ds, img_count_test_real = data_processing.load_dataset(paths_real,\"TEST\", IMG_SIZE_MODEL_INPUT, IMG_SIZE_MODEL_OUTPUT, NUM_CLASSES, MAX_IMG_TEST)\n",
    "test_real_ds = data_processing.dataset_processing(test_real_ds, cache=False, shuffle=False, batch_size=BS_REAL, prefetch=False, img_count = img_count_test_real)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics Evaluation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "if not TRAIN_MODEL:\n",
    "    step_width = 0.025\n",
    "    threshold_range = [0.025, 0.975]\n",
    "    threshold_array = np.arange(threshold_range[0],threshold_range[1]+step_width,step_width)\n",
    "\n",
    "    path_metrics_evaluation_plot = os.path.join(paths[\"FIGURES\"],\"threshold_metrics_evaluation_test_real_edge_threshold_{:.1f}.svg\".format(0))\n",
    "    threshold_f1_max = visualize.plot_threshold_metrics_evaluation_class(model=model, ds=test_real_ds, \n",
    "                                                                   num_classes = NUM_CLASSES,\n",
    "                                                                   threshold_array=threshold_array, \n",
    "                                                                   threshold_edge_width=0, save=SAVE, \n",
    "                                                                   path=path_metrics_evaluation_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual Results"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "if not TRAIN_MODEL:\n",
    "    for img, label in test_real_ds.take(1):\n",
    "        img, label = img, label\n",
    "\n",
    "\n",
    "    threshold = 0.5\n",
    "\n",
    "    predictions = model.predict(img)    \n",
    "    predictions = tools.predict_class_postprocessing(predictions, threshold=threshold)\n",
    "\n",
    "    path = os.path.join(paths[\"FIGURES\"],\"images_test_real_threshold_{:.2f}\".format(threshold))\n",
    "    visualize.plot_images(images=img, labels=label, predictions=predictions, save=SAVE, path=path, batch_size=8)\n",
    "\n",
    "    threshold = threshold_f1_max\n",
    "\n",
    "    predictions = model.predict(img)    \n",
    "    predictions = tools.predict_class_postprocessing(predictions, threshold=threshold)\n",
    "\n",
    "    path = os.path.join(paths[\"FIGURES\"],\"images_test_real_threshold_ods\")\n",
    "    visualize.plot_images(images=img, labels=label, predictions=predictions, save=SAVE, path=path, batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_cfg[\"SAVE\"]:\n",
    "    model.save(MF.paths[\"MODEL\"])\n",
    "    \n",
    "    custom_objects = {\"BinaryAccuracyEdges\": metrics.BinaryAccuracyEdges,\n",
    "                      \"F1Edges\": metrics.F1Edges}\n",
    "    \n",
    "    model = tf.keras.models.load_model(MF.paths[\"MODEL\"], custom_objects=custom_objects)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
